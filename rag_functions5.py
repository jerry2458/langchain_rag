import re
from langchain.prompts import PromptTemplate

# âœ… LLMì„ ì´ìš©í•´ ì¹œì ˆí•œ í•´ì„¤ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def generate_detailed_explanation(llm, question, explanation, answer, user_prompt):
    """
    ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì œ, í•´ì„¤, ì •ë‹µì„ ë°”íƒ•ìœ¼ë¡œ AIê°€ ì¹œì ˆí•œ í•´ì„¤ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    """
    prompt_template = PromptTemplate(
        template=(
            "ë‹¤ìŒì€ HTML íƒœê·¸ê°€ í¬í•¨ëœ ìˆ˜í•™ ë¬¸ì œ, í•´ì„¤, ì •ë‹µ ë°ì´í„°ì…ë‹ˆë‹¤:\n"
            f"ğŸ”¹ ë¬¸ì œ: {question}\n"
            f"ğŸ”¹ ê¸°ì¡´ í•´ì„¤: {explanation}\n"
            f"ğŸ”¹ ì •ë‹µ: {answer}\n"
            f"{user_prompt}"
            "ğŸ’¡ AIê°€ ë³€í™˜í•œ í•´ì„¤:"
        ),
        input_variables=["user_prompt", "question", "explanation", "answer"]
    )

    response = llm.predict(prompt_template.format(
        user_prompt=user_prompt,
        question=question,
        explanation=explanation,
        answer=answer
        )
    )

    return response  
