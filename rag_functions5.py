from langchain.prompts import PromptTemplate
from langchain.schema import HumanMessage

def generate_detailed_explanation(llm, question, explanation, answer, user_prompt):
    """
    ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë¬¸ì œ, í•´ì„¤, ì •ë‹µì„ ë°”íƒ•ìœ¼ë¡œ AIê°€ ì¹œì ˆí•œ í•´ì„¤ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
    """

    # âœ… PromptTemplateì„ ë¬¸ìì—´ë¡œ ë³€í™˜
    prompt_template = PromptTemplate(
        template=(
            "{user_prompt}\n\n"
            "ğŸ”¹ ë¬¸ì œ: {question}\n"
            "ğŸ”¹ ê¸°ì¡´ í•´ì„¤: {explanation}\n"
            "ğŸ”¹ ì •ë‹µ: {answer}\n\n"
            "ğŸ’¡ AIê°€ ë³€í™˜í•œ í•´ì„¤:"
        ),
        input_variables=["user_prompt", "question", "explanation", "answer"]
    )

    formatted_prompt = prompt_template.format(
        user_prompt=user_prompt,
        question=question,
        explanation=explanation,
        answer=answer
    )

    # âœ… LangChainì˜ generate()ì— ì „ë‹¬ë  ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    response = llm.generate([[HumanMessage(content=formatted_prompt)]])

    return response.generations[0][0].text  # âœ… ì‘ë‹µ ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
