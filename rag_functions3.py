import os
import pandas as pd
from langchain.document_loaders import PDFPlumberLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.chains.question_answering import load_qa_chain

# OpenAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["TOKENIZERS_PARALLELISM"] = "false"


__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

# âœ… (1) HTML í˜•ì‹ì˜ ë¬¸ì œ ë° í•´ì„¤ ë°ì´í„° ë¡œë“œ
def load_html_explanation_data(file_path):
    df = pd.read_csv(file_path)  # CSVì—ì„œ HTML í˜•ì‹ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    explanations = []
    for _, row in df.iterrows():
        explanations.append({
            "question": row["ë¬¸ì œ"],
            "explanation": row["í•´ì„¤"]
        })
    return explanations


# âœ… (2) PDF íŒŒì¼ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ë¶„í• 
def load_and_split_pdf(file_path):
    loader = PDFPlumberLoader(file_path)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=100)
    split_docs = text_splitter.split_documents(documents)
    return split_docs


# âœ… (3) ë²¡í„° ì €ì¥ì†Œ ìƒì„± (ë¬¸ì œ í•´ì„¤ + PDF ë‹¨ì› ì €ì¥)
def create_vector_store(explanations, pdf_texts, persist_directory="chroma_db"):
    embeddings = OpenAIEmbeddings()

    # HTML ë¬¸ì œ í•´ì„¤ì„ ChromaDBì— ì¶”ê°€
    explanation_texts = [f"<b>ë¬¸ì œ:</b> {ex['question']}<br><b>í•´ì„¤:</b> {ex['explanation']}" for ex in explanations]
    explanation_store = Chroma.from_texts(explanation_texts, embeddings, persist_directory=persist_directory + "/explanations")

    # PDF ë‹¨ì›ë„ ChromaDBì— ì¶”ê°€
    pdf_store = Chroma.from_documents(pdf_texts, embeddings, persist_directory=persist_directory + "/pdfs")

    return explanation_store, pdf_store


# âœ… (4) RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì²´ì¸ ìƒì„±
def create_rag_chain(explanation_store, pdf_store):
    llm = ChatOpenAI(model_name="gpt-4", temperature=0)

    retriever = explanation_store.as_retriever()  # ë¬¸ì œ í•´ì„¤ ê²€ìƒ‰
    pdf_retriever = pdf_store.as_retriever()  # PDF ê²€ìƒ‰

    prompt_template = PromptTemplate(
        template=(
            "í•™ìƒì˜ ì§ˆë¬¸ì— ëŒ€í•œ í•´ì„¤ê³¼ ê´€ë ¨ ê°œë…ì„ ì œê³µí•©ë‹ˆë‹¤ (HTML í˜•ì‹ ì¶œë ¥):\n\n"
            "ğŸ”¹ <b>ë¬¸ì œ í•´ì„¤</b><br>{context}<br>\n"
            "ğŸ”¹ <b>ê´€ë ¨ ê°œë… PDF ë‹¨ì›</b><br>{pdf_context}<br>\n"
            "ì§ˆë¬¸: {question}\n"
            "LaTeX ìˆ˜ì‹ì´ í¬í•¨ëœ HTML í˜•ì‹ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
        ),
        input_variables=["context", "pdf_context", "question"]
    )

    llm_chain = load_qa_chain(llm, chain_type="stuff", prompt=prompt_template)

    return llm_chain, retriever, pdf_retriever
